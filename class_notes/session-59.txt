cluster created
present blue ng is running
apps are also running

upgrade
------------
send a communication that EKS is getting upgraded, no new deployments and releses happen
change the SG to remove access to other teams

create green ng with same capacity
cordon green nodes
kubectl cordon <node-name> -> scheduling disabled
upgrade control plane in console to 1.31
upgrade green node group also to 1.31

cordon blue nodes
uncordon green nodes

drain blue nodes --> automatically workloads will come to green
delete blue


VM to Containers migration(K8)
-----------------------
existing ALB or new ALB through ingress

ALB --> Listener --> Rule --> Target group(Health checks)(Instance based) --> VM
ALB --> Listener --> Rule --> Target group(Health checks) --> Pods

Create ACM, Create ALB, Listener, Rule, Target Group (IP based)

1. Ingress resource --> target group
2. Target group binding --> adds our pods to target group

eksctl utils associate-iam-oidc-provider \
    --region us-east-1 \
    --cluster expense-dev \
    --approve
	
eksctl create iamserviceaccount \
--cluster=expense-dev \
--namespace=kube-system \
--name=aws-load-balancer-controller \
--attach-policy-arn=arn:aws:iam::315069654700:policy/AWSLoadBalancerControllerIAMPolicy \
--override-existing-serviceaccounts \
--region us-east-1 \
--approve

helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=expense-dev --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller

helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=expense-dev --set serviceAccount.create=true --set serviceAccount.name=aws-load-balancer-controller

